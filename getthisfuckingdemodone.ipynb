{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorpack import tfutils\n",
    "import tensorflow as tf\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MFCC_SIZE = 20\n",
    "N_INP_FRMS = 50\n",
    "\n",
    "utt_data = np.load('exp/test_utt.npy')\n",
    "n_frms = utt_data.shape[0] - N_INP_FRMS + 1\n",
    "batch_x = np.zeros((n_frms, N_INP_FRMS*MFCC_SIZE))\n",
    "for i in range(n_frms):\n",
    "    batch_x[i] = utt_data[i:(i+N_INP_FRMS), 0:MFCC_SIZE].reshape(1, N_INP_FRMS*MFCC_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running network inference...\n",
      "INFO:tensorflow:Restoring parameters from ./model_data/model-1\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = tf.train.latest_checkpoint(\"./model_data/\")\n",
    "saver = tf.train.import_meta_graph(checkpoint_path + \".meta\", import_scope=None)\n",
    "print(\"Running network inference...\")\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    nn_dvector = sess.run(\"dense_1/BiasAdd:0\", feed_dict={\"maxout_dense_1_input:0\": batch_x})\n",
    "    bn1 = sess.run(\"batch_normalization_1/batchnorm_1/add_1:0\", feed_dict={\"maxout_dense_1_input:0\": batch_x})\n",
    "    bn2 = sess.run(\"batch_normalization_2/batchnorm_1/add_1:0\", feed_dict={\"maxout_dense_1_input:0\": batch_x})\n",
    "    bn3 = sess.run(\"batch_normalization_3/batchnorm_1/add_1:0\", feed_dict={\"maxout_dense_1_input:0\": batch_x})\n",
    "    bn4 = sess.run(\"batch_normalization_4/batchnorm_1/add_1:0\", feed_dict={\"maxout_dense_1_input:0\": batch_x})\n",
    "    bn5 = sess.run(\"batch_normalization_5/batchnorm_1/add_1:0\", feed_dict={\"maxout_dense_1_input:0\": batch_x})\n",
    "bns = [bn1,bn2,bn3,bn4,bn5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "vrs = tfutils.varmanip.load_chkpt_vars('model_data/checkpoint')\n",
    "np.save('model_data/model-1.npy', vrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn(inp, var_ema, mean_ema, gamma, beta):\n",
    "    scale = gamma/np.sqrt(var_ema+1e-5)\n",
    "    scaled_inp = inp*scale\n",
    "    scaled_mean_ema = scale*mean_ema\n",
    "    bias = beta-scaled_mean_ema\n",
    "    out = scaled_inp + bias\n",
    "    return out\n",
    "\n",
    "def maxout(W, b, inp):\n",
    "    t1 = np.dot(inp, W[0]) + b[0]\n",
    "    t2 = np.dot(inp, W[1]) + b[1]\n",
    "    t3 = np.dot(inp, W[2]) + b[2]\n",
    "    t4 = np.dot(inp, W[3]) + b[3]\n",
    "    tmax = np.maximum(np.maximum(np.maximum(t1,t2), t3), t4)\n",
    "    return tmax\n",
    "\n",
    "def fc(inp,W,b):\n",
    "    return np.dot(inp,W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "vrs = np.load('model_data/model-1.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying maxout layer: 1\n",
      "Trying maxout layer: 2\n",
      "Trying maxout layer: 3\n",
      "Trying maxout layer: 4\n",
      "Trying maxout layer: 5\n"
     ]
    }
   ],
   "source": [
    "inp = batch_x\n",
    "for i in range(1,6):\n",
    "    print(\"Trying maxout layer:\", i)\n",
    "    W = vrs['maxout_dense_%s/W' % (i,)]\n",
    "    b = vrs['maxout_dense_%s/b' % (i,)]\n",
    "\n",
    "    mx = maxout(W, b, inp)\n",
    "\n",
    "    var_ema = vrs['batch_normalization_%s/moving_variance' % (i,)]\n",
    "    mean_ema = vrs['batch_normalization_%s/moving_mean' % (i,)]\n",
    "    gamma = vrs['batch_normalization_%s/gamma' % (i,)]\n",
    "    beta = vrs['batch_normalization_%s/beta' % (i,)]\n",
    "\n",
    "    inp = bn(mx, var_ema, mean_ema, gamma, beta)\n",
    "    assert np.allclose(inp, bns[i-1], atol=1e-5)\n",
    "\n",
    "W = vrs['dense_1/kernel']\n",
    "b = vrs['dense_1/bias']\n",
    "out = fc(inp, W, b)\n",
    "assert np.allclose(out, nn_dvector, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08466631174087524\n",
      "0.07206255197525024\n",
      "0.08271986246109009\n",
      "0.09243404865264893\n",
      "0.1268584132194519\n",
      "0.07794433832168579\n",
      "0.13270705938339233\n",
      "0.1324428915977478\n",
      "0.10052770376205444\n",
      "0.09876036643981934\n",
      "avg dist 0.10011235475540162\n"
     ]
    }
   ],
   "source": [
    "g_mean = np.average(nn_dvector, axis=0)\n",
    "sum_dists = 0\n",
    "for i in range(10):\n",
    "    m = np.load('model_data/skanda/' + str(i) + '.npy')\n",
    "    m_mean = np.average(m, axis=0)\n",
    "    dist = 1 - np.dot(m_mean, g_mean)/(np.linalg.norm(m_mean)*np.linalg.norm(g_mean))\n",
    "    sum_dists += dist\n",
    "    print(dist)\n",
    "avg_dist = sum_dists/10\n",
    "print('avg dist', avg_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
